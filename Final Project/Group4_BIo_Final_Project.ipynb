{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics Final Project: Group 4\n",
    "### Coral Host taken into consideration is OANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Gene Expression data and the meta data files for the Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0A011QK89</th>\n",
       "      <th>A0A023I7E1</th>\n",
       "      <th>A0A067XMP1</th>\n",
       "      <th>A0A067YMX8</th>\n",
       "      <th>A0A075HNX4</th>\n",
       "      <th>A0A0A0RM07</th>\n",
       "      <th>A0A0B4KGY6</th>\n",
       "      <th>A0A0B6VQ48</th>\n",
       "      <th>A0A0B7P9G0</th>\n",
       "      <th>A0A0G2JDV3</th>\n",
       "      <th>...</th>\n",
       "      <th>S0EMV0</th>\n",
       "      <th>S8GJB7</th>\n",
       "      <th>V6CLA2</th>\n",
       "      <th>W0LYS5</th>\n",
       "      <th>W0TIW1</th>\n",
       "      <th>W7JLR6</th>\n",
       "      <th>W7JX98</th>\n",
       "      <th>W7K9M0</th>\n",
       "      <th>W7NDQ0</th>\n",
       "      <th>X1WER2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M_c1_C</th>\n",
       "      <td>3.907570</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>2.703474</td>\n",
       "      <td>2.986870</td>\n",
       "      <td>2.241169</td>\n",
       "      <td>2.521235</td>\n",
       "      <td>3.692186</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>6.353335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>1.856850</td>\n",
       "      <td>3.104000</td>\n",
       "      <td>3.119761</td>\n",
       "      <td>3.371791</td>\n",
       "      <td>1.856850</td>\n",
       "      <td>2.359892</td>\n",
       "      <td>4.727864</td>\n",
       "      <td>4.686284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M_c3_C</th>\n",
       "      <td>5.441870</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.535626</td>\n",
       "      <td>3.657173</td>\n",
       "      <td>3.478619</td>\n",
       "      <td>2.503367</td>\n",
       "      <td>3.106804</td>\n",
       "      <td>4.619353</td>\n",
       "      <td>4.672777</td>\n",
       "      <td>7.503357</td>\n",
       "      <td>...</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>1.869137</td>\n",
       "      <td>2.976699</td>\n",
       "      <td>3.426356</td>\n",
       "      <td>3.944844</td>\n",
       "      <td>4.232551</td>\n",
       "      <td>2.173351</td>\n",
       "      <td>2.273015</td>\n",
       "      <td>6.026444</td>\n",
       "      <td>6.010146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M_c5_C</th>\n",
       "      <td>5.331988</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.125354</td>\n",
       "      <td>3.570151</td>\n",
       "      <td>3.630813</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>4.273808</td>\n",
       "      <td>5.297177</td>\n",
       "      <td>7.510740</td>\n",
       "      <td>...</td>\n",
       "      <td>1.948261</td>\n",
       "      <td>2.346861</td>\n",
       "      <td>3.570151</td>\n",
       "      <td>2.414528</td>\n",
       "      <td>3.368204</td>\n",
       "      <td>4.119319</td>\n",
       "      <td>2.729975</td>\n",
       "      <td>2.684824</td>\n",
       "      <td>5.862211</td>\n",
       "      <td>5.935485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M_c6_C</th>\n",
       "      <td>5.619051</td>\n",
       "      <td>1.974893</td>\n",
       "      <td>2.605541</td>\n",
       "      <td>3.519751</td>\n",
       "      <td>2.948207</td>\n",
       "      <td>2.404832</td>\n",
       "      <td>3.137391</td>\n",
       "      <td>4.295079</td>\n",
       "      <td>4.585000</td>\n",
       "      <td>7.673688</td>\n",
       "      <td>...</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.544092</td>\n",
       "      <td>2.905994</td>\n",
       "      <td>2.544092</td>\n",
       "      <td>3.662709</td>\n",
       "      <td>4.478651</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.477620</td>\n",
       "      <td>6.219767</td>\n",
       "      <td>5.997715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M_c7_C</th>\n",
       "      <td>5.021363</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.913924</td>\n",
       "      <td>3.443582</td>\n",
       "      <td>3.627241</td>\n",
       "      <td>2.838415</td>\n",
       "      <td>2.937903</td>\n",
       "      <td>4.301555</td>\n",
       "      <td>4.861119</td>\n",
       "      <td>7.184987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.887821</td>\n",
       "      <td>2.152548</td>\n",
       "      <td>3.050253</td>\n",
       "      <td>2.454656</td>\n",
       "      <td>3.666083</td>\n",
       "      <td>3.930574</td>\n",
       "      <td>2.567736</td>\n",
       "      <td>2.214481</td>\n",
       "      <td>5.709837</td>\n",
       "      <td>5.492568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A0A011QK89  A0A023I7E1  A0A067XMP1  A0A067YMX8  A0A075HNX4  \\\n",
       "M_c1_C    3.907570    1.620841    3.004398    2.703474    2.986870   \n",
       "M_c3_C    5.441870    1.620841    2.535626    3.657173    3.478619   \n",
       "M_c5_C    5.331988    1.620841    3.125354    3.570151    3.630813   \n",
       "M_c6_C    5.619051    1.974893    2.605541    3.519751    2.948207   \n",
       "M_c7_C    5.021363    1.620841    2.913924    3.443582    3.627241   \n",
       "\n",
       "        A0A0A0RM07  A0A0B4KGY6  A0A0B6VQ48  A0A0B7P9G0  A0A0G2JDV3  ...  \\\n",
       "M_c1_C    2.241169    2.521235    3.692186    3.004398    6.353335  ...   \n",
       "M_c3_C    2.503367    3.106804    4.619353    4.672777    7.503357  ...   \n",
       "M_c5_C    2.891829    2.891829    4.273808    5.297177    7.510740  ...   \n",
       "M_c6_C    2.404832    3.137391    4.295079    4.585000    7.673688  ...   \n",
       "M_c7_C    2.838415    2.937903    4.301555    4.861119    7.184987  ...   \n",
       "\n",
       "          S0EMV0    S8GJB7    V6CLA2    W0LYS5    W0TIW1    W7JLR6    W7JX98  \\\n",
       "M_c1_C  1.620841  1.620841  1.856850  3.104000  3.119761  3.371791  1.856850   \n",
       "M_c3_C  1.620841  1.869137  2.976699  3.426356  3.944844  4.232551  2.173351   \n",
       "M_c5_C  1.948261  2.346861  3.570151  2.414528  3.368204  4.119319  2.729975   \n",
       "M_c6_C  1.620841  2.544092  2.905994  2.544092  3.662709  4.478651  1.620841   \n",
       "M_c7_C  1.887821  2.152548  3.050253  2.454656  3.666083  3.930574  2.567736   \n",
       "\n",
       "          W7K9M0    W7NDQ0    X1WER2  \n",
       "M_c1_C  2.359892  4.727864  4.686284  \n",
       "M_c3_C  2.273015  6.026444  6.010146  \n",
       "M_c5_C  2.684824  5.862211  5.935485  \n",
       "M_c6_C  2.477620  6.219767  5.997715  \n",
       "M_c7_C  2.214481  5.709837  5.492568  \n",
       "\n",
       "[5 rows x 3484 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Gene Expression Data\n",
    "Gene_expression = pd.read_csv('normalized_counts_vst.csv', index_col = 0).T\n",
    "Gene_expression.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>Clade</th>\n",
       "      <th>Host</th>\n",
       "      <th>Dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M_c1_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M_c3_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M_c5_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M_c6_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M_c7_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sample_ID Clade  Host Dominant\n",
       "0    M_c1_C     C  MCAV      Yes\n",
       "1    M_c3_C     C  MCAV      Yes\n",
       "2    M_c5_C     C  MCAV      Yes\n",
       "3    M_c6_C     C  MCAV       No\n",
       "4    M_c7_C     C  MCAV      Yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the metadata file.\n",
    "metadata = pd.read_csv(\"metadata.csv\")\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0A011QK89</th>\n",
       "      <th>A0A023I7E1</th>\n",
       "      <th>A0A067XMP1</th>\n",
       "      <th>A0A067YMX8</th>\n",
       "      <th>A0A075HNX4</th>\n",
       "      <th>A0A0A0RM07</th>\n",
       "      <th>A0A0B4KGY6</th>\n",
       "      <th>A0A0B6VQ48</th>\n",
       "      <th>A0A0B7P9G0</th>\n",
       "      <th>A0A0G2JDV3</th>\n",
       "      <th>...</th>\n",
       "      <th>W0TIW1</th>\n",
       "      <th>W7JLR6</th>\n",
       "      <th>W7JX98</th>\n",
       "      <th>W7K9M0</th>\n",
       "      <th>W7NDQ0</th>\n",
       "      <th>X1WER2</th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>Clade</th>\n",
       "      <th>Host</th>\n",
       "      <th>Dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.907570</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>2.703474</td>\n",
       "      <td>2.986870</td>\n",
       "      <td>2.241169</td>\n",
       "      <td>2.521235</td>\n",
       "      <td>3.692186</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>6.353335</td>\n",
       "      <td>...</td>\n",
       "      <td>3.119761</td>\n",
       "      <td>3.371791</td>\n",
       "      <td>1.856850</td>\n",
       "      <td>2.359892</td>\n",
       "      <td>4.727864</td>\n",
       "      <td>4.686284</td>\n",
       "      <td>M_c1_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.441870</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.535626</td>\n",
       "      <td>3.657173</td>\n",
       "      <td>3.478619</td>\n",
       "      <td>2.503367</td>\n",
       "      <td>3.106804</td>\n",
       "      <td>4.619353</td>\n",
       "      <td>4.672777</td>\n",
       "      <td>7.503357</td>\n",
       "      <td>...</td>\n",
       "      <td>3.944844</td>\n",
       "      <td>4.232551</td>\n",
       "      <td>2.173351</td>\n",
       "      <td>2.273015</td>\n",
       "      <td>6.026444</td>\n",
       "      <td>6.010146</td>\n",
       "      <td>M_c3_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.331988</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.125354</td>\n",
       "      <td>3.570151</td>\n",
       "      <td>3.630813</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>4.273808</td>\n",
       "      <td>5.297177</td>\n",
       "      <td>7.510740</td>\n",
       "      <td>...</td>\n",
       "      <td>3.368204</td>\n",
       "      <td>4.119319</td>\n",
       "      <td>2.729975</td>\n",
       "      <td>2.684824</td>\n",
       "      <td>5.862211</td>\n",
       "      <td>5.935485</td>\n",
       "      <td>M_c5_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.619051</td>\n",
       "      <td>1.974893</td>\n",
       "      <td>2.605541</td>\n",
       "      <td>3.519751</td>\n",
       "      <td>2.948207</td>\n",
       "      <td>2.404832</td>\n",
       "      <td>3.137391</td>\n",
       "      <td>4.295079</td>\n",
       "      <td>4.585000</td>\n",
       "      <td>7.673688</td>\n",
       "      <td>...</td>\n",
       "      <td>3.662709</td>\n",
       "      <td>4.478651</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.477620</td>\n",
       "      <td>6.219767</td>\n",
       "      <td>5.997715</td>\n",
       "      <td>M_c6_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.021363</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.913924</td>\n",
       "      <td>3.443582</td>\n",
       "      <td>3.627241</td>\n",
       "      <td>2.838415</td>\n",
       "      <td>2.937903</td>\n",
       "      <td>4.301555</td>\n",
       "      <td>4.861119</td>\n",
       "      <td>7.184987</td>\n",
       "      <td>...</td>\n",
       "      <td>3.666083</td>\n",
       "      <td>3.930574</td>\n",
       "      <td>2.567736</td>\n",
       "      <td>2.214481</td>\n",
       "      <td>5.709837</td>\n",
       "      <td>5.492568</td>\n",
       "      <td>M_c7_C</td>\n",
       "      <td>C</td>\n",
       "      <td>MCAV</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3488 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A0A011QK89  A0A023I7E1  A0A067XMP1  A0A067YMX8  A0A075HNX4  A0A0A0RM07  \\\n",
       "0    3.907570    1.620841    3.004398    2.703474    2.986870    2.241169   \n",
       "1    5.441870    1.620841    2.535626    3.657173    3.478619    2.503367   \n",
       "2    5.331988    1.620841    3.125354    3.570151    3.630813    2.891829   \n",
       "3    5.619051    1.974893    2.605541    3.519751    2.948207    2.404832   \n",
       "4    5.021363    1.620841    2.913924    3.443582    3.627241    2.838415   \n",
       "\n",
       "   A0A0B4KGY6  A0A0B6VQ48  A0A0B7P9G0  A0A0G2JDV3  ...    W0TIW1    W7JLR6  \\\n",
       "0    2.521235    3.692186    3.004398    6.353335  ...  3.119761  3.371791   \n",
       "1    3.106804    4.619353    4.672777    7.503357  ...  3.944844  4.232551   \n",
       "2    2.891829    4.273808    5.297177    7.510740  ...  3.368204  4.119319   \n",
       "3    3.137391    4.295079    4.585000    7.673688  ...  3.662709  4.478651   \n",
       "4    2.937903    4.301555    4.861119    7.184987  ...  3.666083  3.930574   \n",
       "\n",
       "     W7JX98    W7K9M0    W7NDQ0    X1WER2  Sample_ID  Clade  Host  Dominant  \n",
       "0  1.856850  2.359892  4.727864  4.686284     M_c1_C      C  MCAV       Yes  \n",
       "1  2.173351  2.273015  6.026444  6.010146     M_c3_C      C  MCAV       Yes  \n",
       "2  2.729975  2.684824  5.862211  5.935485     M_c5_C      C  MCAV       Yes  \n",
       "3  1.620841  2.477620  6.219767  5.997715     M_c6_C      C  MCAV        No  \n",
       "4  2.567736  2.214481  5.709837  5.492568     M_c7_C      C  MCAV       Yes  \n",
       "\n",
       "[5 rows x 3488 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging the Gene Expression and Meta data together.\n",
    "data = pd.merge(Gene_expression, metadata, left_index = True, right_on = 'Sample_ID')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0A011QK89</th>\n",
       "      <th>A0A023I7E1</th>\n",
       "      <th>A0A067XMP1</th>\n",
       "      <th>A0A067YMX8</th>\n",
       "      <th>A0A075HNX4</th>\n",
       "      <th>A0A0A0RM07</th>\n",
       "      <th>A0A0B4KGY6</th>\n",
       "      <th>A0A0B6VQ48</th>\n",
       "      <th>A0A0B7P9G0</th>\n",
       "      <th>A0A0G2JDV3</th>\n",
       "      <th>...</th>\n",
       "      <th>S8GJB7</th>\n",
       "      <th>V6CLA2</th>\n",
       "      <th>W0LYS5</th>\n",
       "      <th>W0TIW1</th>\n",
       "      <th>W7JLR6</th>\n",
       "      <th>W7JX98</th>\n",
       "      <th>W7K9M0</th>\n",
       "      <th>W7NDQ0</th>\n",
       "      <th>X1WER2</th>\n",
       "      <th>Host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.907570</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>2.703474</td>\n",
       "      <td>2.986870</td>\n",
       "      <td>2.241169</td>\n",
       "      <td>2.521235</td>\n",
       "      <td>3.692186</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>6.353335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>1.856850</td>\n",
       "      <td>3.104000</td>\n",
       "      <td>3.119761</td>\n",
       "      <td>3.371791</td>\n",
       "      <td>1.856850</td>\n",
       "      <td>2.359892</td>\n",
       "      <td>4.727864</td>\n",
       "      <td>4.686284</td>\n",
       "      <td>MCAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.441870</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.535626</td>\n",
       "      <td>3.657173</td>\n",
       "      <td>3.478619</td>\n",
       "      <td>2.503367</td>\n",
       "      <td>3.106804</td>\n",
       "      <td>4.619353</td>\n",
       "      <td>4.672777</td>\n",
       "      <td>7.503357</td>\n",
       "      <td>...</td>\n",
       "      <td>1.869137</td>\n",
       "      <td>2.976699</td>\n",
       "      <td>3.426356</td>\n",
       "      <td>3.944844</td>\n",
       "      <td>4.232551</td>\n",
       "      <td>2.173351</td>\n",
       "      <td>2.273015</td>\n",
       "      <td>6.026444</td>\n",
       "      <td>6.010146</td>\n",
       "      <td>MCAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.331988</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.125354</td>\n",
       "      <td>3.570151</td>\n",
       "      <td>3.630813</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>4.273808</td>\n",
       "      <td>5.297177</td>\n",
       "      <td>7.510740</td>\n",
       "      <td>...</td>\n",
       "      <td>2.346861</td>\n",
       "      <td>3.570151</td>\n",
       "      <td>2.414528</td>\n",
       "      <td>3.368204</td>\n",
       "      <td>4.119319</td>\n",
       "      <td>2.729975</td>\n",
       "      <td>2.684824</td>\n",
       "      <td>5.862211</td>\n",
       "      <td>5.935485</td>\n",
       "      <td>MCAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.619051</td>\n",
       "      <td>1.974893</td>\n",
       "      <td>2.605541</td>\n",
       "      <td>3.519751</td>\n",
       "      <td>2.948207</td>\n",
       "      <td>2.404832</td>\n",
       "      <td>3.137391</td>\n",
       "      <td>4.295079</td>\n",
       "      <td>4.585000</td>\n",
       "      <td>7.673688</td>\n",
       "      <td>...</td>\n",
       "      <td>2.544092</td>\n",
       "      <td>2.905994</td>\n",
       "      <td>2.544092</td>\n",
       "      <td>3.662709</td>\n",
       "      <td>4.478651</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.477620</td>\n",
       "      <td>6.219767</td>\n",
       "      <td>5.997715</td>\n",
       "      <td>MCAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.021363</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.913924</td>\n",
       "      <td>3.443582</td>\n",
       "      <td>3.627241</td>\n",
       "      <td>2.838415</td>\n",
       "      <td>2.937903</td>\n",
       "      <td>4.301555</td>\n",
       "      <td>4.861119</td>\n",
       "      <td>7.184987</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152548</td>\n",
       "      <td>3.050253</td>\n",
       "      <td>2.454656</td>\n",
       "      <td>3.666083</td>\n",
       "      <td>3.930574</td>\n",
       "      <td>2.567736</td>\n",
       "      <td>2.214481</td>\n",
       "      <td>5.709837</td>\n",
       "      <td>5.492568</td>\n",
       "      <td>MCAV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3485 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A0A011QK89  A0A023I7E1  A0A067XMP1  A0A067YMX8  A0A075HNX4  A0A0A0RM07  \\\n",
       "0    3.907570    1.620841    3.004398    2.703474    2.986870    2.241169   \n",
       "1    5.441870    1.620841    2.535626    3.657173    3.478619    2.503367   \n",
       "2    5.331988    1.620841    3.125354    3.570151    3.630813    2.891829   \n",
       "3    5.619051    1.974893    2.605541    3.519751    2.948207    2.404832   \n",
       "4    5.021363    1.620841    2.913924    3.443582    3.627241    2.838415   \n",
       "\n",
       "   A0A0B4KGY6  A0A0B6VQ48  A0A0B7P9G0  A0A0G2JDV3  ...    S8GJB7    V6CLA2  \\\n",
       "0    2.521235    3.692186    3.004398    6.353335  ...  1.620841  1.856850   \n",
       "1    3.106804    4.619353    4.672777    7.503357  ...  1.869137  2.976699   \n",
       "2    2.891829    4.273808    5.297177    7.510740  ...  2.346861  3.570151   \n",
       "3    3.137391    4.295079    4.585000    7.673688  ...  2.544092  2.905994   \n",
       "4    2.937903    4.301555    4.861119    7.184987  ...  2.152548  3.050253   \n",
       "\n",
       "     W0LYS5    W0TIW1    W7JLR6    W7JX98    W7K9M0    W7NDQ0    X1WER2  Host  \n",
       "0  3.104000  3.119761  3.371791  1.856850  2.359892  4.727864  4.686284  MCAV  \n",
       "1  3.426356  3.944844  4.232551  2.173351  2.273015  6.026444  6.010146  MCAV  \n",
       "2  2.414528  3.368204  4.119319  2.729975  2.684824  5.862211  5.935485  MCAV  \n",
       "3  2.544092  3.662709  4.478651  1.620841  2.477620  6.219767  5.997715  MCAV  \n",
       "4  2.454656  3.666083  3.930574  2.567736  2.214481  5.709837  5.492568  MCAV  \n",
       "\n",
       "[5 rows x 3485 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the column that are not taken into consideration.\n",
    "data = data.drop(['Sample_ID','Clade', 'Dominant' ],axis = 1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0A011QK89</th>\n",
       "      <th>A0A023I7E1</th>\n",
       "      <th>A0A067XMP1</th>\n",
       "      <th>A0A067YMX8</th>\n",
       "      <th>A0A075HNX4</th>\n",
       "      <th>A0A0A0RM07</th>\n",
       "      <th>A0A0B4KGY6</th>\n",
       "      <th>A0A0B6VQ48</th>\n",
       "      <th>A0A0B7P9G0</th>\n",
       "      <th>A0A0G2JDV3</th>\n",
       "      <th>...</th>\n",
       "      <th>S8GJB7</th>\n",
       "      <th>V6CLA2</th>\n",
       "      <th>W0LYS5</th>\n",
       "      <th>W0TIW1</th>\n",
       "      <th>W7JLR6</th>\n",
       "      <th>W7JX98</th>\n",
       "      <th>W7K9M0</th>\n",
       "      <th>W7NDQ0</th>\n",
       "      <th>X1WER2</th>\n",
       "      <th>Host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.907570</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>2.703474</td>\n",
       "      <td>2.986870</td>\n",
       "      <td>2.241169</td>\n",
       "      <td>2.521235</td>\n",
       "      <td>3.692186</td>\n",
       "      <td>3.004398</td>\n",
       "      <td>6.353335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>1.856850</td>\n",
       "      <td>3.104000</td>\n",
       "      <td>3.119761</td>\n",
       "      <td>3.371791</td>\n",
       "      <td>1.856850</td>\n",
       "      <td>2.359892</td>\n",
       "      <td>4.727864</td>\n",
       "      <td>4.686284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.441870</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.535626</td>\n",
       "      <td>3.657173</td>\n",
       "      <td>3.478619</td>\n",
       "      <td>2.503367</td>\n",
       "      <td>3.106804</td>\n",
       "      <td>4.619353</td>\n",
       "      <td>4.672777</td>\n",
       "      <td>7.503357</td>\n",
       "      <td>...</td>\n",
       "      <td>1.869137</td>\n",
       "      <td>2.976699</td>\n",
       "      <td>3.426356</td>\n",
       "      <td>3.944844</td>\n",
       "      <td>4.232551</td>\n",
       "      <td>2.173351</td>\n",
       "      <td>2.273015</td>\n",
       "      <td>6.026444</td>\n",
       "      <td>6.010146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.331988</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>3.125354</td>\n",
       "      <td>3.570151</td>\n",
       "      <td>3.630813</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>2.891829</td>\n",
       "      <td>4.273808</td>\n",
       "      <td>5.297177</td>\n",
       "      <td>7.510740</td>\n",
       "      <td>...</td>\n",
       "      <td>2.346861</td>\n",
       "      <td>3.570151</td>\n",
       "      <td>2.414528</td>\n",
       "      <td>3.368204</td>\n",
       "      <td>4.119319</td>\n",
       "      <td>2.729975</td>\n",
       "      <td>2.684824</td>\n",
       "      <td>5.862211</td>\n",
       "      <td>5.935485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.619051</td>\n",
       "      <td>1.974893</td>\n",
       "      <td>2.605541</td>\n",
       "      <td>3.519751</td>\n",
       "      <td>2.948207</td>\n",
       "      <td>2.404832</td>\n",
       "      <td>3.137391</td>\n",
       "      <td>4.295079</td>\n",
       "      <td>4.585000</td>\n",
       "      <td>7.673688</td>\n",
       "      <td>...</td>\n",
       "      <td>2.544092</td>\n",
       "      <td>2.905994</td>\n",
       "      <td>2.544092</td>\n",
       "      <td>3.662709</td>\n",
       "      <td>4.478651</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.477620</td>\n",
       "      <td>6.219767</td>\n",
       "      <td>5.997715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.021363</td>\n",
       "      <td>1.620841</td>\n",
       "      <td>2.913924</td>\n",
       "      <td>3.443582</td>\n",
       "      <td>3.627241</td>\n",
       "      <td>2.838415</td>\n",
       "      <td>2.937903</td>\n",
       "      <td>4.301555</td>\n",
       "      <td>4.861119</td>\n",
       "      <td>7.184987</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152548</td>\n",
       "      <td>3.050253</td>\n",
       "      <td>2.454656</td>\n",
       "      <td>3.666083</td>\n",
       "      <td>3.930574</td>\n",
       "      <td>2.567736</td>\n",
       "      <td>2.214481</td>\n",
       "      <td>5.709837</td>\n",
       "      <td>5.492568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3485 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A0A011QK89  A0A023I7E1  A0A067XMP1  A0A067YMX8  A0A075HNX4  A0A0A0RM07  \\\n",
       "0    3.907570    1.620841    3.004398    2.703474    2.986870    2.241169   \n",
       "1    5.441870    1.620841    2.535626    3.657173    3.478619    2.503367   \n",
       "2    5.331988    1.620841    3.125354    3.570151    3.630813    2.891829   \n",
       "3    5.619051    1.974893    2.605541    3.519751    2.948207    2.404832   \n",
       "4    5.021363    1.620841    2.913924    3.443582    3.627241    2.838415   \n",
       "\n",
       "   A0A0B4KGY6  A0A0B6VQ48  A0A0B7P9G0  A0A0G2JDV3  ...    S8GJB7    V6CLA2  \\\n",
       "0    2.521235    3.692186    3.004398    6.353335  ...  1.620841  1.856850   \n",
       "1    3.106804    4.619353    4.672777    7.503357  ...  1.869137  2.976699   \n",
       "2    2.891829    4.273808    5.297177    7.510740  ...  2.346861  3.570151   \n",
       "3    3.137391    4.295079    4.585000    7.673688  ...  2.544092  2.905994   \n",
       "4    2.937903    4.301555    4.861119    7.184987  ...  2.152548  3.050253   \n",
       "\n",
       "     W0LYS5    W0TIW1    W7JLR6    W7JX98    W7K9M0    W7NDQ0    X1WER2  Host  \n",
       "0  3.104000  3.119761  3.371791  1.856850  2.359892  4.727864  4.686284     0  \n",
       "1  3.426356  3.944844  4.232551  2.173351  2.273015  6.026444  6.010146     0  \n",
       "2  2.414528  3.368204  4.119319  2.729975  2.684824  5.862211  5.935485     0  \n",
       "3  2.544092  3.662709  4.478651  1.620841  2.477620  6.219767  5.997715     0  \n",
       "4  2.454656  3.666083  3.930574  2.567736  2.214481  5.709837  5.492568     0  \n",
       "\n",
       "[5 rows x 3485 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Coral HOST into numericals by making HOST == OANN as 1 and HOST != OANN as 0.\n",
    "data['Host'] = (data['Host'] == 'OANN').astype(int)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HOST is our Target and the rest of the data is the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column\n",
    "y = data['Host']\n",
    "# Inputs of the target column\n",
    "X = data.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Accuracy: 0.9714285714285715\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation \n",
    "# Defining the cross validation accuracy.\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Cross Validation Accuracy with a fold of 5.\n",
    "cv_score = cross_val_score(lr, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "print(\"Logistic Regression CV Accuracy:\", cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model with all parameters and testing the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Testing Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# FItting the model.\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the results on the testing set.4\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# FInding the Accurayc of the logistic Reqgression Model.\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Regression Testing Accuracy:\", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the confusion Matrix of the predictions and finding the Error rate fo the Predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+----------------------+\n",
      "| Confusion Matrix:   |   Predicted Positive |   Predicted Negative |\n",
      "+=====================+======================+======================+\n",
      "| Actual Positive     |                    7 |                    0 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "| Actual Negative     |                    1 |                    1 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "Error Rate: 0.1111\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are already defined\n",
    "cons_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the error rate\n",
    "Error_rate = (cons_matrix[0, 1] + cons_matrix[1, 0]) / cons_matrix.sum()\n",
    "\n",
    "# Prepare table data\n",
    "table_data = [\n",
    "    [\"Actual Positive\", cons_matrix[0, 0], cons_matrix[0, 1]],\n",
    "    [\"Actual Negative\", cons_matrix[1, 0], cons_matrix[1, 1]]\n",
    "]\n",
    "\n",
    "# Defining the headers\n",
    "headers = [\"Confusion Matrix:\", \"Predicted Positive\", \"Predicted Negative\"]\n",
    "\n",
    "print(tabulate(table_data, headers, tablefmt = \"grid\"))\n",
    "print(f\"Error Rate: {Error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the feature importance for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of significant features required for the resutls within in top 32% important features : 458\n",
      "The number of significant features required for the resutls within in top 5% important features : 179\n",
      "Top 10 features contributing to the results:-\n",
      "['Q8ZRS8', 'P46285', 'Q43011', 'Q9SF16', 'Q0V6P9', 'Q53RK8', 'Q94B38', 'Q0WKV3', 'Q28057', 'Q3ZBH0']\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients and corresponding feature names\n",
    "coefficients = lr.coef_[0]\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to display coefficients and feature names\n",
    "coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Display the DataFrame sorted by absolute coefficient values\n",
    "coefficients_df['Absolute Coefficient'] = coefficients_df['Coefficient'].abs()\n",
    "sorted_coefficients = coefficients_df.sort_values(by='Absolute Coefficient', ascending = False)\n",
    "\n",
    "# Calculate the mean and standard deviation of the absolute coefficients\n",
    "mean_coeff = np.mean(sorted_coefficients['Absolute Coefficient'].abs())\n",
    "std_coeff = np.std(sorted_coefficients['Absolute Coefficient'].abs())\n",
    "\n",
    "# Setting the threshold\n",
    "threshold = mean_coeff + std_coeff\n",
    "\n",
    "# Select features with absolute coefficients greater than the threshold\n",
    "significant_features = sorted_coefficients[sorted_coefficients['Absolute Coefficient'].abs() > threshold]\n",
    "print(f\"The number of significant features required for the resutls within in top 32% important features : {len(significant_features)}\")\n",
    "\n",
    "# Setting the threshold\n",
    "threshold = mean_coeff + std_coeff*2\n",
    "# Select features with absolute coefficients greater than the threshold\n",
    "significant_features = sorted_coefficients[sorted_coefficients['Absolute Coefficient'].abs() > threshold]\n",
    "print(f\"The number of significant features required for the resutls within in top 5% important features : {len(significant_features)}\")\n",
    "\n",
    "# Top 10 important features\n",
    "print(\"Top 10 features contributing to the results:-\")\n",
    "top_features = sorted_coefficients['Feature'].head(10)\n",
    "print(top_features.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FitFailedWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = [\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga'], 'max_iter': [100, 200, 300]},\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['elasticnet'], 'solver': ['saga'], 'max_iter': [100, 200, 300]}\n",
    "]\n",
    "\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and the corresponding score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model with the best parameters and predicting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Testing Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# Creating the logistic regression object.\n",
    "lr = LogisticRegression(C= 0.01, max_iter= 1000, penalty= 'l2', solver= 'liblinear')\n",
    "\n",
    "# FItting the model.\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the results on the testing set.4\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# FInding the Accurayc of the logistic Reqgression Model.\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Regression Testing Accuracy:\", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Questions \n",
    "\n",
    "### 1) How Many Genes/Components Are Obtained/Required Significant Results? \n",
    "  In this question, based on your algorithm, are there a certain number of genes or components (depending on the algorithm) that can perform the classification of your problem? \n",
    "\n",
    "- The number of significant features required for the resutls within in top 32% important features : 458\n",
    "- The number of significant features required for the resutls within in top 5% important features : 179\n",
    "- Top 10 features contributing to the results can do the classification:-\n",
    "- ['Q8ZRS8', 'P46285', 'Q43011', 'Q9SF16', 'Q0V6P9', 'Q53RK8', 'Q94B38', 'Q0WKV3', 'Q28057', 'Q3ZBH0']\n",
    "\n",
    "### 2) What is the Error Rate of These Algorithms? \n",
    "  In this question, based on your algorithms, how much of the test dataset is misclassified?\n",
    "\n",
    "- Error Rate is 0.111 for the test dataset. Only 1 is misclassified.\n",
    "  \n",
    "### 3) What are two genes that were signficiantly correlated to your problem? \n",
    "  In this question, based on your algorithm, identify and annotate 2 genes using [uniprot](uniprot.org). Do you think they are biologically relevant? \n",
    "\n",
    "- Aconitate Hydratase B (AcnB) from Salmonella typhimurium (Q8ZRS8 · ACNB_SALTY):\n",
    "\n",
    "Function: Involved in the catabolism of short-chain fatty acids via the tricarboxylic acid (TCA) cycle and the 2-methylcitrate cycle. It catalyzes the reversible isomerization of citrate to isocitrate and the hydration of 2-methyl-cis-aconitate.\n",
    "Biological Relevance: The TCA cycle is a key metabolic pathway that provides energy and precursors for biosynthesis. In the context of Cladacopium, this enzyme could be crucial for energy metabolism. The ability to process various carbon sources, including short-chain fatty acids, might be significant in the symbiotic relationship, possibly affecting the nutrient exchange dynamics.\n",
    "\n",
    "- Sedoheptulose-1,7-bisphosphatase, Chloroplastic from Triticum aestivum (Wheat) (P46285 · S17P_WHEAT):\n",
    "\n",
    "Function: This enzyme plays a role in the Calvin cycle, which is involved in photosynthesis.\n",
    "Biological Relevance: In relation to Cladacopium, which are photosynthetic algae living in coral tissues, this enzyme could be significant for photosynthesis and carbon fixation. Its role in the Calvin cycle implies its importance in converting carbon dioxide into organic compounds, which is vital for the symbiotic relationship in providing nutrients to the coral host.\n",
    "\n",
    "- Given their functions in crucial metabolic pathways, both enzymes are likely to be biologically relevant in the context of Cladacopium, especially considering the metabolic and photosynthetic processes essential for the symbiosis with coral hosts. \n",
    "\n",
    "### 4) What can we infer about this dataset based on these results? \n",
    "  In this question, you should think about the overall classification results of this algorithm. Does the type of algorithm used here tell you anything about the data in this study? What is biologically meaningful about this? \n",
    "\n",
    "- From the results 88% of accuracy on test data, 97.14% Cross validation accuracy and the predictions of Logistic Regression algorithm used can provide insights into whether the focus is on expression patterns, functional annotations, or evolutionary relationships, each offering different perspectives on the biological significance of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing cross validation on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis Cross-validated Accuracy: 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "# Initializing the LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Performing a  5-fold cross-validation on the training data to evaluate the model's performance\n",
    "cv_score = cross_val_score(lda, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the mean cross-validated accuracy score\n",
    "print(\"Linear Discriminant Analysis Cross-validated Accuracy:\", cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and predicting the results on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis Test data Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# Fitting the LDA model to the training data\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy score on the test set\n",
    "# 'y_test' are the true labels for the test data\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Linear Discriminant Analysis Test data Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding different matrices of evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Values:\n",
      " [-22.51474059 -21.3001699   -5.5964112  -24.98414154 -33.95100689\n",
      " -24.87532512  21.31513501 -16.56494812 -23.70147123]\n",
      "\n",
      "Predicted Probabilities:\n",
      " [[1.00000000e+00 1.66714126e-10]\n",
      " [9.99999999e-01 5.61634463e-10]\n",
      " [9.96302563e-01 3.69743668e-03]\n",
      " [1.00000000e+00 1.41099409e-11]\n",
      " [1.00000000e+00 1.79996912e-15]\n",
      " [1.00000000e+00 1.57319865e-11]\n",
      " [5.53292079e-10 9.99999999e-01]\n",
      " [9.99999936e-01 6.39638226e-08]\n",
      " [1.00000000e+00 5.08840687e-11]]\n",
      "\n",
      "Predicted Classes:\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.89         9\n",
      "   macro avg       0.94      0.75      0.80         9\n",
      "weighted avg       0.90      0.89      0.87         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the decision function value for each sample in the test set.\n",
    "# This value indicates the distance of each sample from the decision boundary.\n",
    "lda_DV = lda.decision_function(X_test)\n",
    "print(\"\\nDecision Values:\\n\", lda_DV)\n",
    "\n",
    "# Calculating the probability of each class for each sample in the test set.\n",
    "# These probabilities show the model's confidence in assigning each class to each sample.\n",
    "lda_PP = lda.predict_proba(X_test)\n",
    "print(\"\\nPredicted Probabilities:\\n\", lda_PP)\n",
    "\n",
    "# Predicting the class labels for the test set.\n",
    "# These are the final classifications made by the model for each sample.\n",
    "lda_PC = lda.predict(X_test)\n",
    "print(\"\\nPredicted Classes:\\n\", lda_PC)\n",
    "\n",
    "# Generating a classification report that includes key metrics like precision, recall, and F1-score.\n",
    "# This report provides a detailed analysis of the model's performance for each class.\n",
    "lda_CR = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", lda_CR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confustion Matrix of the LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+----------------------+\n",
      "| Confusion Matrix:   |   Predicted Positive |   Predicted Negative |\n",
      "+=====================+======================+======================+\n",
      "| Actual Positive     |                    7 |                    0 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "| Actual Negative     |                    1 |                    1 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "Error Rate: 0.1111\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are already defined\n",
    "cons_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the error rate\n",
    "Error_rate = (cons_matrix[0, 1] + cons_matrix[1, 0]) / cons_matrix.sum()\n",
    "\n",
    "# Prepare table data\n",
    "table_data = [\n",
    "    [\"Actual Positive\", cons_matrix[0, 0], cons_matrix[0, 1]],\n",
    "    [\"Actual Negative\", cons_matrix[1, 0], cons_matrix[1, 1]]\n",
    "]\n",
    "\n",
    "# Defining the headers\n",
    "headers = [\"Confusion Matrix:\", \"Predicted Positive\", \"Predicted Negative\"]\n",
    "\n",
    "print(tabulate(table_data, headers, tablefmt = \"grid\"))\n",
    "print(f\"Error Rate: {Error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)\n",
    "- RFE is a technique for selecting features by recursively considering smaller and smaller sets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0A075HNX4', 'A5FKJ2', 'B4GUZ2', 'B7GJ95', 'O30124', 'P07335', 'P22694', 'P32019', 'P32811', 'P51574', 'P9WKM1', 'Q0J9V3', 'Q23551', 'Q54FZ8', 'Q54GE3', 'Q54HL8', 'Q5M9G6', 'Q5RCS8', 'Q5UPD4', 'Q5YLM1', 'Q5ZKR7', 'Q68FR8', 'Q6GNI4', 'Q6ICB0', 'Q6P9J9', 'Q6S5H5', 'Q7RJG2', 'Q80ZA4', 'Q86WI1', 'Q86XH1', 'Q8GYH8', 'Q8L611', 'Q8L716', 'Q949Y0', 'Q9BSJ2', 'Q9BXP8', 'Q9EPK2', 'Q9HDZ5', 'Q9M271', 'Q9UJZ1']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Initialize LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Creating a list of the features using the columns of X\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Initialize RFE with the LDA model and the desired number of features\n",
    "rfe = RFE(lda, n_features_to_select = 40)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get the ranking of the features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "# Select the top features based on ranking\n",
    "top_features = [features[i] for i in range(len(features)) if feature_ranking[i] == 1]\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most influential features using the coefficient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most influential genes: [('Q54GE3', 0.32893503837190824), ('B9F676', 0.3269164164157803), ('P54399', 0.30848877233608185), ('Q0VCN3', 0.2829772938120041), ('Q9HDZ5', 0.22735039343467034), ('Q96S16', 0.21421552854237969), ('A3LNF8', 0.20495138631202564), ('A1CUK5', 0.20481453412744383), ('O94264', 0.20306847313909698), ('Q55G18', 0.19048664760273734)]\n"
     ]
    }
   ],
   "source": [
    "# Creating the LDA object and fitting the model.\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Get the absolute coefficients for each gene\n",
    "coefficients = abs(lda.coef_[0])  # Assuming binary classification for simplicity\n",
    "\n",
    "# Combine gene names with their coefficients\n",
    "gene_importance = list(zip(features, coefficients))\n",
    "\n",
    "# Sorting the genes by their importance\n",
    "sorted_genes = sorted(gene_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the most influential genes\n",
    "print(\"Most influential genes:\", sorted_genes[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the number of genes that are significant to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of significant genes present for the resutls within in top 32% important features : 310\n",
      "The number of significant genes present for the resutls within in top 5% important features : 105\n",
      "Top 10 features : ['Q54GE3', 'B9F676', 'P54399', 'Q0VCN3', 'Q9HDZ5', 'Q96S16', 'A3LNF8', 'A1CUK5', 'O94264', 'Q55G18']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of the absolute coefficients\n",
    "mean_coeff = np.mean(coefficients)\n",
    "std_coeff = np.std(coefficients)\n",
    "\n",
    "# Setting the threshold\n",
    "threshold = mean_coeff + std_coeff\n",
    "\n",
    "# Extract genes with importance score above the threshold\n",
    "genes_above_threshold = [gene[0] for gene in sorted_genes if gene[1] > threshold]\n",
    "\n",
    "# Selecting features with absolute coefficients greater than the threshold\n",
    "print(f\"The number of significant genes present for the resutls within in top 32% important features : {len(genes_above_threshold)}\")\n",
    "\n",
    "# Setting the threshold\n",
    "threshold = mean_coeff + std_coeff*2\n",
    "# Selecting features with absolute coefficients greater than the threshold\n",
    "genes_above_threshold = [gene[0] for gene in sorted_genes if gene[1] > threshold]\n",
    "print(f\"The number of significant genes present for the resutls within in top 5% important features : {len(genes_above_threshold)}\")\n",
    "print(\"Top 10 features :\", genes_above_threshold[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q54GE3 and Q9HDZ5 are the two genes that are common in the most influential features and the features that are found by Recursive Feature Elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "### How Many Genes/Components Are Obtained/Required Significant Results? \n",
    "  In this question, based on your algorithm, are there a certain number of genes or components (depending on the algorithm) that can perform the classification of your problem? \n",
    "\n",
    "- The number of significant genes present for the resutls within in top 32% important features : 310\n",
    "- The number of significant genes present for the resutls within in top 5% important features : 105\n",
    "- Top 10 features that contribute to the classification are:- \n",
    "- ['Q54GE3', 'B9F676', 'P54399', 'Q0VCN3', 'Q9HDZ5', 'Q96S16', 'A3LNF8', 'A1CUK5', 'O94264', 'Q55G18']\n",
    "\n",
    "### What is the Error Rate of These Algorithms? \n",
    "  In this question, based on your algorithms, how much of the test dataset is misclassified?  \n",
    "- The Error Rate is similar to the results of the Logistic Regression. Only 1 is misclassified.\n",
    "  \n",
    "### What are two genes that were signficiantly correlated to your problem? \n",
    "  In this question, based on your algorithm, identify and annotate 2 genes using [uniprot](uniprot.org). Do you think they are biologically relevant? \n",
    "\n",
    "- Q54GE3 · VPS45_DICDI: \n",
    "- Involved in vesicle-mediated protein trafficking, particularly in the Golgi stack and trans-Golgi network. This process is crucial for cellular functioning, as it is involved in sorting and transporting proteins to their destination.\n",
    "\n",
    "- Q9HDZ5 · YKP9_SCHPO:\n",
    "- CRAL-TRIO domain-containing protein C589.09: Predicted to be mitochondrial, indicating a role in cellular energy metabolism or other mitochondrial functions. - - The CRAL-TRIO domain is often involved in binding small lipophilic molecules, suggesting a potential role in lipid metabolism or signaling.\n",
    "\n",
    "- While the direct relevance of these specific proteins to Cladacopium and its coral host might not be immediately clear, the functional roles they play in fundamental cellular processes could offer indirect insights into the biological mechanisms underlying symbiosis. We are unclear about the biologically direct relationship.\n",
    "\n",
    "### What can we infer about this dataset based on these results? \n",
    "  In this question, you should think about the overall classification results of this algorithm. Does the type of algorithm used here tell you anything about the data in this study? What is biologically meaningful about this?\n",
    "\n",
    "- The overall classification was similar to the results of the Logistic Regression with an Accuracy of 88% but the Cross validation accuracy of 94.28% is less than Logistic Regression. The identification of VPS45 and CRAL-TRIO domain-containing protein C589.09, and the use of LDA, suggest a focus on distinguishing cellular states or types based on fundamental biological processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation score with SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Cross-validated Accuracy: 0.9714285714285715\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Support Vector Machine classifier with linear kernel and degree 5\n",
    "svm = SVC(kernel='linear', degree=5)\n",
    "\n",
    "# Performing 5-fold cross-validation to estimate the accuracy of the SVM\n",
    "cv_scores = cross_val_score(svm, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Support Vector Machine Cross-validated Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model using SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes Used for Prediction:\n",
      " ['A0A023I7E1', 'A0A067XMP1', 'A0A0A0RM07', 'A0A0G2JDV3', 'A0A0G2JEB6', 'A0A0H2VDN9', 'A0A0I9QGZ2', 'A0A0K9RDW0', 'A0A0R4IBK5', 'A0A125YZN2', 'A0A1B4XBG9', 'A0A1D8PDZ1', 'A0A4V8H042', 'A0A075HNX4', 'A0A0B4KGY6', 'A0A0B6VQ48']\n"
     ]
    }
   ],
   "source": [
    "# Fitting the SVM model to the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Retrieving the indices of the support vectors\n",
    "support_vector_indices = svm.support_\n",
    "\n",
    "# Extracting the gene names corresponding to the support vector indices\n",
    "selected_genes = X_train.columns[support_vector_indices]\n",
    "print(\"Genes Used for Prediction:\\n\", selected_genes.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Test Set Accuracy: 0.8888888888888888\n",
      "Number of Components (Support Vectors): 16\n"
     ]
    }
   ],
   "source": [
    "# Predicting the labels for the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Support Vector Machine Test Set Accuracy:\", test_accuracy)\n",
    "\n",
    "# Displaying the number of support vectors used in the model\n",
    "print(\"Number of Components (Support Vectors):\", svm.support_vectors_.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning of SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV 1/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .......C=0.1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .......C=0.1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .......C=0.1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .......C=0.1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .C=0.1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .C=0.1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .C=0.1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .C=0.1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .........C=1, gamma=1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .........C=1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .........C=1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .........C=1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .........C=1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ...C=1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ...C=1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ...C=1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=1, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 1/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .....C=1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .....C=1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=1, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END ........C=10, gamma=1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ........C=10, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ........C=10, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ........C=10, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ........C=10, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ......C=10, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ......C=10, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ......C=10, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ......C=10, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=10, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .....C=10, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .....C=10, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=10, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ..C=10, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ..C=10, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ..C=10, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ..C=10, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ....C=10, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ....C=10, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ....C=10, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ....C=10, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .......C=100, gamma=1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .......C=100, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .......C=100, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .......C=100, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .......C=100, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .....C=100, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .....C=100, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .....C=100, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=100, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ....C=100, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ....C=100, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ....C=100, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ....C=100, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .C=100, gamma=0.001, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END .C=100, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END .C=100, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END .C=100, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .C=100, gamma=0.001, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END ...C=100, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 3/5] END ...C=100, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 4/5] END ...C=100, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=100, gamma=0.001, kernel=poly;, score=1.000 total time=   0.0s\n",
      "Best Parameters: {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Creating a GridSearchCV object to find the best parameters\n",
    "grid_search = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv = 5)\n",
    "\n",
    "# Fitting the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Printing the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the best Parameters from teh Hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned SVM Test Set Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# Predicting with the best estimator\n",
    "y_pred_tuned = grid_search.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy on the test set with the tuned parameters\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "print(\"Tuned SVM Test Set Accuracy:\", tuned_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+----------------------+\n",
      "| Confusion Matrix:   |   Predicted Positive |   Predicted Negative |\n",
      "+=====================+======================+======================+\n",
      "| Actual Positive     |                    7 |                    0 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "| Actual Negative     |                    1 |                    1 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "Error Rate: 0.1111\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are already defined\n",
    "cons_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the error rate\n",
    "Error_rate = (cons_matrix[0, 1] + cons_matrix[1, 0]) / cons_matrix.sum()\n",
    "\n",
    "# Prepare table data\n",
    "table_data = [\n",
    "    [\"Actual Positive\", cons_matrix[0, 0], cons_matrix[0, 1]],\n",
    "    [\"Actual Negative\", cons_matrix[1, 0], cons_matrix[1, 1]]\n",
    "]\n",
    "\n",
    "# Defining the headers\n",
    "headers = [\"Confusion Matrix:\", \"Predicted Positive\", \"Predicted Negative\"]\n",
    "\n",
    "print(tabulate(table_data, headers, tablefmt = \"grid\"))\n",
    "print(f\"Error Rate: {Error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the most significant genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 Significant Genes:\n",
      " [('Q8ZRS8', 0.003722035964487068), ('Q9SF16', 0.0036126978014409186)]\n",
      "Gene: Q8ZRS8, Weight: 0.003722035964487068\n",
      "Gene: Q9SF16, Weight: 0.0036126978014409186\n"
     ]
    }
   ],
   "source": [
    "# Finding the coefficients of SVM.\n",
    "coefficients = svm.coef_[0]\n",
    "\n",
    "# Ranking the genes by the absolute values of the coefficients\n",
    "gene_importance = sorted(zip(X_train.columns, coefficients), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Selecting the top 2 significant genes\n",
    "top_genes = gene_importance[:2]\n",
    "print(\"Top 2 Significant Genes:\\n\", top_genes)\n",
    "\n",
    "# Annotate these genes\n",
    "for gene, weight in top_genes:\n",
    "    # Perform a literature search or database query for each gene\n",
    "    # Annotate the biological relevance based on your findings\n",
    "    print(f\"Gene: {gene}, Weight: {weight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "### How Many Genes/Components Are Obtained/Required Significant Results? \n",
    "  In this question, based on your algorithm, are there a certain number of genes or components (depending on the algorithm) that can perform the classification of your problem? \n",
    "\n",
    "- For the Support Vector machines 16 components can perform the classification of your problem.\n",
    "- The genes used for Prediction are 'A0A023I7E1', 'A0A067XMP1', 'A0A0A0RM07', 'A0A0G2JDV3', 'A0A0G2JEB6', 'A0A0H2VDN9', 'A0A0I9QGZ2', 'A0A0K9RDW0', 'A0A0R4IBK5', 'A0A125YZN2', 'A0A1B4XBG9', 'A0A1D8PDZ1', 'A0A4V8H042', 'A0A075HNX4', 'A0A0B4KGY6', 'A0A0B6VQ48'\n",
    "\n",
    "### What is the Error Rate of These Algorithms? \n",
    "  In this question, based on your algorithms, how much of the test dataset is misclassified? \n",
    "\n",
    "- The Error Rate of the SVM is 0.1111 and only 1 value is misclassified.\n",
    "\n",
    "### What are two genes that were signficiantly correlated to your problem? \n",
    "  In this question, based on your algorithm, identify and annotate 2 genes using [uniprot](uniprot.org). Do you think they are biologically relevant? \n",
    "\n",
    "T-complex Protein 1 Subunit Eta (CCT7) - Arabidopsis thaliana ( Q9SF16 · TCPH_ARATH ):\n",
    "\n",
    "- Function: Acts as a molecular chaperone, assisting in the folding of proteins upon ATP hydrolysis. It plays a role in the folding of actin and tubulin, which are crucial for cellular structure and dynamics.\n",
    "- Relevance to Cladacopium: In the context of Cladacopium, protein folding and structural integrity are essential for maintaining cellular function, especially under stress conditions prevalent in symbiotic environments. Molecular chaperones like CCT7 might be crucial in ensuring the proper functioning of proteins that sustain the photosynthetic and metabolic activities of the algae.\n",
    "\n",
    "Aconitate Hydratase B (AcnB) - Salmonella typhimurium ( Q8ZRS8 · ACNB_SALTY ):\n",
    "\n",
    "- Function: Involved in the catabolism of short-chain fatty acids through the TCA cycle and the 2-methylcitrate cycle. It plays a role in isomerizing citrate to isocitrate and in the hydration of 2-methyl-cis-aconitate.\n",
    "- Relevance to Cladacopium: The TCA cycle is fundamental for energy metabolism. In Cladacopium, efficient energy metabolism is crucial for sustaining the symbiotic relationship with coral hosts. The ability to process various carbon sources might impact the nutrient exchange and energy dynamics within the symbiosis.\n",
    "\n",
    "### What can we infer about this dataset based on these results? \n",
    "  In this question, you should think about the overall classification results of this algorithm. Does the type of algorithm used here tell you anything about the data in this study? What is biologically meaningful about this? \n",
    "\n",
    "- The Support vector machines are able to perform with an Cross validation accuracy of 97.14% which is good. The fact that these proteins Q8ZRS8 · ACNB_SALTY and Q9SF16 · TCPH_ARATH were highlighted as significant features in an SVM analysis suggests they might have distinctive roles or expressions in certain conditions or states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation using Partial Least Squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Least Square Cross-validated Accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Partial Least Squares Regression model with 5 components\n",
    "pls = PLSRegression(n_components=5) \n",
    "\n",
    "# Performing cross-validation\n",
    "# Cross-validating the model using 5 folds and predicting the response for the training set\n",
    "y_pred_cross_val = cross_val_predict(pls, X_train, y_train, cv=5)\n",
    "\n",
    "# Setting a threshold for classification\n",
    "threshold = 0.5 \n",
    "\n",
    "# Converting predictions to binary format based on the threshold\n",
    "y_pred_cross_val_binary = (y_pred_cross_val > threshold).astype(int)\n",
    "\n",
    "# Calculating the accuracy of the cross-validated predictions\n",
    "cross_val_accuracy = accuracy_score(y_train, y_pred_cross_val_binary)\n",
    "print(\"Partial Least Square Cross-validated Accuracy:\", cross_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and finding the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Least Square Test Set Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model to the training data\n",
    "pls.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the response for the test set\n",
    "y_pred_test = pls.predict(X_test)\n",
    "\n",
    "# Converting test predictions to binary format based on the threshold\n",
    "y_pred_test_binary = (y_pred_test > threshold).astype(int)\n",
    "\n",
    "# Calculating the accuracy for the test set predictions\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test_binary)\n",
    "print(\"Partial Least Square Test Set Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the top features for the Partial least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Genes for PLS Method: Index(['B9F676', 'Q0VCN3', 'P46285', 'Q55G18', 'Q53RK8'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Extracting the loadings for the first component\n",
    "loadings_component_1 = pls.x_weights_[:, 0]\n",
    "\n",
    "# Identifying the indices of the top genes\n",
    "top_gene_indices = loadings_component_1.argsort()[-5:][::-1]\n",
    "\n",
    "# Selecting the top genes based on the loadings\n",
    "top_genes = X_train.columns[top_gene_indices]\n",
    "print(\"Top Genes for PLS Method:\", top_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and the Error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+----------------------+\n",
      "| Confusion Matrix:   |   Predicted Positive |   Predicted Negative |\n",
      "+=====================+======================+======================+\n",
      "| Actual Positive     |                    7 |                    0 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "| Actual Negative     |                    1 |                    1 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "Error Rate for Partial Least Square: 0.11111111111111116\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are already defined\n",
    "cons_matrix = confusion_matrix(y_test, y_pred_test_binary)\n",
    "\n",
    "# Calculate the error rate\n",
    "Error_rate = (cons_matrix[0, 1] + cons_matrix[1, 0]) / cons_matrix.sum()\n",
    "\n",
    "# Prepare table data\n",
    "table_data = [\n",
    "    [\"Actual Positive\", cons_matrix[0, 0], cons_matrix[0, 1]],\n",
    "    [\"Actual Negative\", cons_matrix[1, 0], cons_matrix[1, 1]]\n",
    "]\n",
    "\n",
    "# Defining the headers\n",
    "headers = [\"Confusion Matrix:\", \"Predicted Positive\", \"Predicted Negative\"]\n",
    "\n",
    "# Printing the results\n",
    "print(tabulate(table_data, headers, tablefmt = \"grid\"))\n",
    "\n",
    "# Calculating the error rate\n",
    "error_rate = 1 - test_accuracy\n",
    "print(f\"Error Rate for Partial Least Square: {error_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "### How Many Genes/Components Are Obtained/Required Significant Results? \n",
    "  In this question, based on your algorithm, are there a certain number of genes or components (depending on the algorithm) that can perform the classification of your problem? \n",
    "\n",
    "- Here top 5 genes that contribute to the classification of your problem are 'B9F676', 'Q0VCN3', 'P46285', 'Q55G18', 'Q53RK8'.\n",
    "\n",
    "### What is the Error Rate of These Algorithms? \n",
    "  In this question, based on your algorithms, how much of the test dataset is misclassified? \n",
    "\n",
    "- Error Rate is similar as the previous algorithms on the test dataset. ie, 0.1116 and only 1 is misclassified.\n",
    "  \n",
    "### What are two genes that were signficiantly correlated to your problem? \n",
    "  In this question, based on your algorithm, identify and annotate 2 genes using [uniprot](uniprot.org). Do you think they are biologically relevant? \n",
    "\n",
    "- Intraflagellar Transport Protein 27 Homolog (IFT27) - Bos taurus (Q0VCN3 · IFT27_BOVIN):\n",
    "\n",
    "- Function: IFT27 is involved in the intraflagellar transport complex, playing a role in cilia formation and function, essential for male fertility, spermiogenesis, sperm flagella formation, and kidney development. It also has a role in hedgehog signaling.\n",
    "\n",
    "- Relevance to Cladacopium: Cilia are not a prominent feature of algae like Cladacopium or their typical coral hosts. However, the broader role of IFT27 in signaling pathways like hedgehog signaling could be relevant in understanding cell communication and developmental processes in these organisms. The exact relevance would require specific investigation.\n",
    "\n",
    "- Probable Glucan 1,3-alpha-glucosidase - Oryza sativa (B9F676 · GLU2A_ORYSJ):\n",
    "\n",
    "- Function: This enzyme is involved in modifying glycoproteins and might play a role in the plant's defense response. It cleaves glucose residues from oligosaccharide precursors of immature glycoproteins.\n",
    "\n",
    "- Relevance to Cladacopium: While plants and algae share some metabolic pathways, the specific role of a glucosidase like this in Cladacopium or its coral host is less clear. However, enzymes involved in glycoprotein processing or in defense responses could be relevant in the context of stress responses or immune-like reactions in the symbiotic relationship.\n",
    "\n",
    "### What can we infer about this dataset based on these results? \n",
    "  In this question, you should think about the overall classification results of this algorithm. Does the type of algorithm used here tell you anything about the data in this study? What is biologically meaningful about this? \n",
    "- The overall classification cross validation results of 93.75% of partical least squares suggests that these proteins were identified based on their correlation with certain outcomes or variables in the study. While IFT27 and glucan 1,3-alpha-glucosidase play important roles in their respective organisms, their direct relevance to Cladacopium and its coral host is not immediately clear and would require more specific research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Removing highly correlated features and removing features with constant values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the absolute correlation matrix for the dataset excluding the 'Host' column\n",
    "corr_matrix = data.drop('Host', axis = 1).corr().abs()\n",
    "\n",
    "# Creating an upper triangle matrix from the correlation matrix\n",
    "# This helps in focusing only on the pairs of features once, since the correlation matrix is symmetric\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Identifying columns to drop based on high correlation\n",
    "# Any column with a correlation greater than 0.9 with another column is marked for removal\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "# Dropping the highly correlated columns from the dataset\n",
    "data_filtered = data.drop(columns=to_drop)\n",
    "\n",
    "# Constant features are those that have the same value in all rows, thus having no variance\n",
    "# Here, such features are removed to improve model performance and reduce overfitting\n",
    "data_filtered = data_filtered.loc[:, data_filtered.apply(pd.Series.nunique) != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the new train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "y = data_filtered['Host']\n",
    "X = data_filtered.drop(['Host'], axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction Technique (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used in the prediction: 32\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Determine the number of components to retain a certain percentage of variance (e.g., 95%)\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumulative_explained_variance >= 0.65) + 1\n",
    "\n",
    "# Apply PCA again with the selected number of components\n",
    "n_components = min(X_train.shape[0], X_train.shape[1])\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"Number of features used in the prediction:\", n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and finding the test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Accuracy: 0.8143\n",
      "Test Set Accuracy: 0.8889\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92         7\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.89         9\n",
      "   macro avg       0.83      0.93      0.86         9\n",
      "weighted avg       0.93      0.89      0.90         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing Quadratic Discriminant Analysis (QDA)\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Performing 5-fold cross-validation on the QDA model using the PCA-transformed training data\n",
    "cv_scores = cross_val_score(qda, X_train_pca, y_train, cv = 5, scoring='accuracy')\n",
    "print(\"Cross-validated Accuracy:\", round(cv_scores.mean(), 4))\n",
    "\n",
    "# Fitting the QDA model to the PCA-transformed training data\n",
    "qda.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predicting the class labels for the PCA-transformed test data using the trained QDA model\n",
    "y_pred = qda.predict(X_test_pca)\n",
    "\n",
    "# Calculating and printing the accuracy of the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", round(test_accuracy, 4))\n",
    "\n",
    "# Generating a classification report that includes key metrics like precision, recall, and F1-score\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and finding the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+----------------------+\n",
      "| Confusion Matrix:   |   Predicted Positive |   Predicted Negative |\n",
      "+=====================+======================+======================+\n",
      "| Actual Positive     |                    6 |                    1 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "| Actual Negative     |                    0 |                    2 |\n",
      "+---------------------+----------------------+----------------------+\n",
      "Error Rate for Quadratic Discriminant Analysis: 0.11111111111111116\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are already defined\n",
    "cons_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the error rate\n",
    "Error_rate = (cons_matrix[0, 1] + cons_matrix[1, 0]) / cons_matrix.sum()\n",
    "\n",
    "# Prepare table data\n",
    "table_data = [\n",
    "    [\"Actual Positive\", cons_matrix[0, 0], cons_matrix[0, 1]],\n",
    "    [\"Actual Negative\", cons_matrix[1, 0], cons_matrix[1, 1]]\n",
    "]\n",
    "\n",
    "# Defining the headers\n",
    "headers = [\"Confusion Matrix:\", \"Predicted Positive\", \"Predicted Negative\"]\n",
    "\n",
    "# Printing the results\n",
    "print(tabulate(table_data, headers, tablefmt = \"grid\"))\n",
    "\n",
    "# Calculating the error rate\n",
    "error_rate = 1 - test_accuracy\n",
    "print(f\"Error Rate for Quadratic Discriminant Analysis: {error_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Top positively correlated genes and most influential genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 positively correlated genes:\n",
      "Index(['Q54GE3', 'O94264'], dtype='object')\n",
      "\n",
      "Top 2 negatively correlated genes:\n",
      "Index(['P23403', 'O44001'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Q3BAI2',\n",
       " 'B5LVL2',\n",
       " 'A7SDW5',\n",
       " 'P23489',\n",
       " 'Q9NES8',\n",
       " 'P23403',\n",
       " 'A6MVW9',\n",
       " 'Q17QZ3',\n",
       " 'Q7ZU45',\n",
       " 'Q1XDP2']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate correlation coefficients for each gene\n",
    "correlation_values = data.drop('Host', axis=1).apply(lambda gene: pearsonr(gene, data['Host'])[0])\n",
    "\n",
    "# Finding the top 2 positively and negatively correlated genes\n",
    "top_pos_correlated_genes = correlation_values.abs().nlargest(2).index\n",
    "top_neg_correlated_genes = correlation_values.abs().nsmallest(2).index\n",
    "\n",
    "# Printing the top 2 gene names\n",
    "print(\"Top 2 positively correlated genes:\")\n",
    "print(top_pos_correlated_genes)\n",
    "\n",
    "print(\"\\nTop 2 negatively correlated genes:\")\n",
    "print(top_neg_correlated_genes)\n",
    "\n",
    "# Finding the most influential genes from the first principal component\n",
    "most_influential_genes = pd.DataFrame({'Gene': X.columns, 'Weight': pca.components_[0]})\n",
    "most_influential_genes = most_influential_genes.sort_values(by = 'Weight', ascending = False)\n",
    "display(most_influential_genes[:10]['Gene'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 'P23403'is common to both the most influential genes and top 2 negatively correlated geneses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "### How Many Genes/Components Are Obtained/Required Significant Results? \n",
    "  In this question, based on your algorithm, are there a certain number of genes or components (depending on the algorithm) that can perform the classification of your problem? \n",
    "\n",
    "- There are 32 features involved in performing the classificatin of our problem. The top 10 features that are contributing to the classification of our problem are :- ['Q3BAI2','B5LVL2','A7SDW5','P23489','Q9NES8','P23403','A6MVW9','Q17QZ3','Q7ZU45','Q1XDP2']\n",
    "\n",
    "### What is the Error Rate of These Algorithms? \n",
    "  In this question, based on your algorithms, how much of the test dataset is misclassified? \n",
    "\n",
    "- The error rate is similar to the other algorithms 0.111 but it failed to predict a relationship that was actually existed which is more harmful.\n",
    "  \n",
    "### What are two genes that were signficiantly correlated to your problem? \n",
    "  In this question, based on your algorithm, identify and annotate 2 genes using [uniprot](uniprot.org). Do you think they are biologically relevant? \n",
    "\n",
    "  - Small Ribosomal Subunit Protein uS10 - Xenopus laevis (P23403 · RS20_XENLA):\n",
    "\n",
    "- Function: This protein is a component of the small ribosomal subunit, which plays a critical role in the process of protein synthesis in cells.\n",
    "\n",
    "- Relevance to Cladacopium: Ribosomes are universal and essential in all living organisms, including Cladacopium and its coral host. The ribosomal machinery is fundamental for protein synthesis, which is vital for the growth, maintenance, and stress response of both symbiotic partners.\n",
    "\n",
    "- Uncharacterized Protein ORF91 - Phalaenopsis aphrodite subsp. formosana (Q3BAI2 · YCX91_PHAAO):\n",
    "\n",
    "- Function: This protein is uncharacterized, and its function is not known. However, the fact that it is listed suggests it might have some distinctive features or importance.\n",
    "\n",
    "- Relevance to Cladacopium: Without specific functional information the biological relavance of this protein cannot be determined.\n",
    "\n",
    "### What can we infer about this dataset based on these results? \n",
    "  In this question, you should think about the overall classification results of this algorithm. Does the type of algorithm used here tell you anything about the data in this study? What is biologically meaningful about this? \n",
    "\n",
    "- The Quadratic Discriminant Analysis performs poorly compared to the other algorithms with an overall cross validation accuracy of 81.43% accuracy. QDA, a statistical classification technique, suggests these proteins were identified based on their ability to discriminate between different classes or groups in the dataset. This might imply that these proteins, or their homologs, play a role in differentiating between certain physiological or environmental states relevant to Cladacopium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result \n",
    "## Which algorithm works best for your problem? Why? \n",
    "  In this question, justify which algorithm you think is the best. Reasoning could include statistics, error rates, or biologically meaningful information. \n",
    "\n",
    "- From the overall results obtained the SVM is the best algorithm for our problem which provides an cross validation accuracy of 97% and provides similar results in the test set. The error of all the algorithms on the test set were similar. Due to high dimensiality of the data the SVM works well in such situations.The fact that these proteins Q8ZRS8 · ACNB_SALTY and Q9SF16 · TCPH_ARATH were highlighted as significant features in an SVM analysis suggests they might have distinctive roles or expressions in certain conditions or states. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
